{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "process = psutil.Process(os.getpid())\n",
    "t = process.memory_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(842641408, 88780800)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.vms, t.rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mem_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / psutil.virtual_memory().total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.005377140343735092"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. [TQDM](https://github.com/tqdm/tqdm) gives you progress bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# Without TQDM\n",
    "s = 0\n",
    "for i in range(10):\n",
    "    s += i\n",
    "    sleep(0.2)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# With TQDM\n",
    "from tqdm import tqdm\n",
    "\n",
    "s = 0\n",
    "for i in tqdm(range(10)):\n",
    "    s += i\n",
    "    sleep(0.2)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, numpy as np, pickle\n",
    "from bz2 import BZ2File\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy import sparse\n",
    "\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "array([4, 6])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S  = sparse.csr_matrix(np.array([[1,2],[3,4]]))\n",
    "Sr= S.sum(axis=0).A1\n",
    "Sr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "array([4, 6, 4, 6])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take(Sr, S.indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 'page_links_en.nt.bz2', please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = 'data/dbpedia/'\n",
    "URL_BASE = 'http://downloads.dbpedia.org/3.5.1/en/'\n",
    "filenames = [\"redirects_en.nt.bz2\", \"page_links_en.nt.bz2\"]\n",
    "\n",
    "for filename in filenames:\n",
    "    if not os.path.exists(PATH+filename):\n",
    "        print(\"Downloading '%s', please wait...\" % filename)\n",
    "        open(PATH+filename, 'wb').write(urlopen(URL_BASE+filename).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redirects_filename = PATH+filenames[0]\n",
    "page_links_filename = PATH+filenames[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Adjacency Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One line of the file looks like:\n",
    "- `<http://dbpedia.org/resource/AfghanistanHistory> <http://dbpedia.org/property/redirect> <http://dbpedia.org/resource/History_of_Afghanistan> .`\n",
    "\n",
    "In the below slice, the plus 1, -1 are to remove the <>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DBPEDIA_RESOURCE_PREFIX_LEN = len(\"http://dbpedia.org/resource/\")\n",
    "SLICE = slice(DBPEDIA_RESOURCE_PREFIX_LEN + 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lines(filename): return (line.split() for line in BZ2File(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through redirections and create dictionary of source to final destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_redirect(targ, redirects):\n",
    "    seen = set()\n",
    "    while True:\n",
    "        transitive_targ = targ\n",
    "        targ = redirects.get(targ)\n",
    "        if targ is None or targ in seen: break\n",
    "        seen.add(targ)\n",
    "    return transitive_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_redirects(redirects_filename):\n",
    "    redirects={}\n",
    "    lines = get_lines(redirects_filename)\n",
    "    return {src[SLICE]:get_redirect(targ[SLICE], redirects) \n",
    "                for src,_,targ,_ in tqdm_notebook(lines, leave=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirects = get_redirects(redirects_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_item(lst, redirects, index_map, item):\n",
    "    k = item[SLICE]\n",
    "    lst.append(index_map.setdefault(redirects.get(k, k), len(index_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit=119077682 #5000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the integer index map\n",
    "index_map = dict() # links->IDs\n",
    "lines = get_lines(page_links_filename)\n",
    "source, destination, data = [],[],[]\n",
    "for l, split in tqdm_notebook(enumerate(lines), total=limit):\n",
    "    if l >= limit: break\n",
    "    add_item(source, redirects, index_map, split[0])\n",
    "    add_item(destination, redirects, index_map, split[2])\n",
    "    data.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(data); n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map.popitem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i,x in enumerate(source) if x == 9991173]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source[119077649], destination[119077649]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to check which page is the source (has index $9991050$).  Note: usually you should not access a dictionary by searching for its values.  This is inefficient and not how dictionaries are intended to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_name, index in index_map.items():\n",
    "    if index == 9991050:\n",
    "        print(page_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see on Wikipedia that the Cincinati Red Teams Issue has [redirected to W711-2](https://en.wikipedia.org/wiki/W711-2):\n",
    "\n",
    "<img src=\"images/cincinnati_reds.png\" alt=\"\" style=\"width: 70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_inds = [i for i,x in enumerate(source) if x == 9991050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dests = [destination[i] for i in test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to check which page is the source (has index 9991174):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_name, index in index_map.items():\n",
    "    if index in test_dests:\n",
    "        print(page_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a sparse matrix using Scipy's COO format, and that convert it to CSR.\n",
    "\n",
    "**Questions**: What are COO and CSR?  Why would we create it with COO and then convert it right away?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sparse.coo_matrix((data, (destination,source)), shape=(n,n), dtype=np.float32)\n",
    "X = X.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(data,destination, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = {i: name for name, i in index_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save matrix so we don't have to recompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(X, open(PATH+'X.pkl', 'wb'))\n",
    "pickle.dump(index_map, open(PATH+'index_map.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pickle.load(open(PATH+'X.pkl', 'rb'))\n",
    "index_map = pickle.load(open(PATH+'index_map.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = {i: name for name, i in index_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $n \\times n$ matrix $A$ is **diagonalizable** if it has $n$ linearly independent eigenvectors $v_1,\\, \\ldots v_n$.\n",
    "\n",
    "Then any $w$ can be expressed $w = \\sum_{j=1}^n c_j v_j $, for some scalars $c_j$.\n",
    "\n",
    "**Exercise:** Show that $$ A^k w = \\sum_{j=1}^n c_j \\lambda_j^k v_j$$\n",
    "\n",
    "**Question**: How will this behave for large $k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is inspiration for the **power method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_ex(v):\n",
    "    print(', '.join(names[i].decode() for i in np.abs(v.squeeze()).argsort()[-1:-10:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?np.squeeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to normalize a sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = sparse.csr_matrix(np.array([[1,2],[3,4]]))\n",
    "Sr = S.sum(axis=0).A1; Sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.data / np.take(Sr, S.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def power_method(A, max_iter=100):\n",
    "    n = A.shape[1]\n",
    "    A = np.copy(A)\n",
    "    A.data /= np.take(A.sum(axis=0).A1, A.indices)\n",
    "\n",
    "    scores = np.ones(n, dtype=np.float32) * np.sqrt(A.sum()/(n*n)) # initial guess\n",
    "    for i in range(max_iter):\n",
    "        scores = A @ scores\n",
    "        nrm = np.linalg.norm(scores)\n",
    "        scores /= nrm\n",
    "        print(nrm)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why normalize the scores on each iteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = power_method(X, max_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_ex(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compare to SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time U, s, V = randomized_svd(X, 3, n_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Top wikipedia pages according to principal singular vectors\n",
    "show_ex(U.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_ex(U.T[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_ex(V[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_ex(V[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the power method to find the eigenvector corresponding to the largest eigenvalue of our matrix of Wikipedia links.  This eigenvector gave us the relative importance of each Wikipedia page (like a simplified PageRank).\n",
    "\n",
    "Next, let's look at a method for finding all eigenvalues of a symmetric, positive definite matrix.  This method includes 2 fundamental algorithms in numerical linear algebra, and is a basis for many more complex methods.\n",
    "\n",
    "[The Second Eigenvalue of the Google Matrix](https://nlp.stanford.edu/pubs/secondeigenvalue.pdf): has \"implications for the convergence rate of the standard PageRank algorithm as the web scales, for the stability of PageRank to perturbations to the link structure of the web, for the detection of Google spammers, and for the design of algorithms to speed up PageRank\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Confusion: QR Algorithm vs QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **QR algorithm** uses something called the **QR decomposition**.  Both are important, so don't get them confused.  The **QR decomposition** decomposes a matrix $A = QR$ into a set of orthonormal columns $Q$ and a triangular matrix $R$.  We will look at several ways to calculate the QR decomposition in a future lesson.  For now, just know that it is giving us an orthogonal matrix and a triangular matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two matrices $A$ and $B$ are **similar** if there exists a non-singular matrix $X$ such that $$B = X^{-1}AX$$\n",
    "\n",
    "Watch this: [Change of Basis](https://www.youtube.com/watch?v=P2LTAUO1TdA&index=13&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "\n",
    "**Theorem**: If $X$ is non-singular, then $A$ and $X^{-1}AX$ have the same eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Schur factorization** of a matrix $A$ is a factorization:\n",
    "$$ A = Q T Q^*$$\n",
    "where $Q$ is unitary and $T$ is upper-triangular.\n",
    "\n",
    "**Question**: What can you say about the eigenvalues of $A$?\n",
    "\n",
    "**Theorem:** Every square matrix has a Schur factorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Other resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Review: [Linear combinations, span, and basis vectors](https://www.youtube.com/watch?v=k7RM-ot2NWY&index=3&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n",
    "\n",
    "See Lecture 24 for proofs of above theorems (and more!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic version of the QR algorithm:\n",
    "\n",
    "    for k=1,2,...\n",
    "        Q, R = A\n",
    "        A = R @ Q\n",
    "        \n",
    "Under suitable assumptions, this algorithm converges to the Schur form of A!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written again, only with subscripts:\n",
    "\n",
    "$A_0 = A$\n",
    "\n",
    "for $k=1,2,\\ldots$\n",
    "\n",
    "   $\\quad Q_k$, $R_k$ = $A_{k-1}$\n",
    "    \n",
    "   $\\quad A_k$ = $R_k Q_k$\n",
    "        \n",
    "We can think of this as constructing sequences of $A_k$, $Q_k$, and $R_k$.\n",
    "\n",
    "$$ A_k = Q_k \\, R_k $$\n",
    "\n",
    "$$ Q_k^{-1} \\, A_k = R_k$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$ R_k Q_k = Q_k^{-1} \\, A_k \\, Q_k $$\n",
    "\n",
    "$$A_k = Q_k^{-1} Q_2^{-1} Q_1^{-1} A Q_1 Q_2 \\dots Q_k$$\n",
    "\n",
    "Trefethen proves the following on page 216-217:\n",
    "\n",
    "$$A^k = Q_1 Q_2 \\dots Q_k R_k R_{k-1}\\dots R_1$$\n",
    "\n",
    "**Key**: The QR algorithm constructs orthonormal bases for successive powers $A^k$.  And remember the close relationship between powers of A and the eigen decomposition.\n",
    "\n",
    "To learn more, read up on *Rayleigh quotients*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pure QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 6\n",
    "A = np.random.rand(n,n)\n",
    "AT = A @ A.T\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pure_qr(A, max_iter=50000):\n",
    "    Ak = np.copy(A)\n",
    "    n = A.shape[0]\n",
    "    QQ = np.eye(n)\n",
    "    for k in range(max_iter):\n",
    "        Q, R = np.linalg.qr(Ak)\n",
    "        Ak = R @ Q\n",
    "        QQ = QQ @ Q\n",
    "        #if k % 100 == 0:\n",
    "        #    print(Ak)\n",
    "         #   print(\"\\n\")\n",
    "    return Ak, QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Ak, QQ = pure_qr(A)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "QQ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pure QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ak, Q = pure_qr(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that Q is orthogonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(np.eye(n), Q @ Q.T), np.allclose(np.eye(n), Q.T @ Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really really slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Practical QR (QR with shifts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Idea**: Instead of factoring $A_k$ as $Q_k R_k$, \n",
    "\n",
    "1. Get the QR factorization $$A_k - s_k I = Q_k R_k$$\n",
    "2. Set $$A_{k+1} = R_k Q_k + s_k I$$\n",
    "\n",
    "Choose $s_k$ to approximate an eigenvalue of $A$.  We'll use $s_k = A_k(m,m)$. \n",
    "\n",
    "The idea of adding shifts to speed up convergence shows up in many algorithms in numerical linear algebra (including the power method, inverse iteration, and Rayleigh quotient iteration).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework: Add shifts to the QR algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exercise: Add shifts to the QR algorithm\n",
    "#Exercise: def practical_qr(A, iters=10):\n",
    "#Exercise:     return Ak, Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ak, Q = practical_qr(A, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that Q is orthogonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(np.eye(n), Q @ Q.T), np.allclose(np.eye(n), Q.T @ Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: This is better than the unshifted version (which wasn't even guaranteed to converge), but is still really slow!  In fact, it is $\\mathcal{O}(n^4)$, which is awful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of symmetric matrices, it's $\\mathcal{O}(n^3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you start with a **Hessenberg matrix** (zeros below the first subdiagonal), it's faster: $\\mathcal{O}(n^3)$, and $\\mathcal{O}(n^2)$ if symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## A Two-Phase Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In practice, a two phase approach is used to find eigenvalues:\n",
    "\n",
    "1. Reduce the matrix to *Hessenberg* form (zeros below the first subdiagonal)\n",
    "2. Iterative process that causes Hessenberg to converge to a *triangular* matrix.  The eigenvalues of a triangular matrix are the values on the diagonal, so we are finished!\n",
    "\n",
    "<img src=\"images/nonhermitian_eigen.JPG\" alt=\"2 phase approach\" style=\"width: 80%\"/>\n",
    "(source: Trefethen, Lecture 25)\n",
    "\n",
    "In the case of a Hermitian matrix, this approach is even faster, since the intermediate step is also Hermitian (and a Hermitian Hessenberg is *tridiagonal*).\n",
    "\n",
    "<img src=\"images/hermitian_eigen.JPG\" alt=\"2 phase approach\" style=\"width: 80%\"/>\n",
    "(source: Trefethen, Lecture 25)\n",
    "\n",
    "Phase 1 reaches an exact solution in a finite number of steps, whereas Phase 2 theoretically never reaches the exact solution.\n",
    "\n",
    "We've already done step 2: the QR algorithm.  Remember that it would be possible to just use the QR algorithm, but ridiculously slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Arnoldi Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can use the Arnoldi iteration for phase 1 (and the QR algorithm for phase 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n = 5\n",
    "A0 = np.random.rand(n,n)  #.astype(np.float64)\n",
    "A = A0 @ A0.T\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Linear Algebra Review: Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When vector $\\mathbf{b}$ is projected onto a line $\\mathbf{a}$, its projection $\\mathbf{p}$ is the part of $\\mathbf{b}$ along that line $\\mathbf{a}$.\n",
    "\n",
    "Let's look at interactive graphic (3.4) for [section 3.2.2: Projections](http://immersivemath.com/ila/ch03_dotproduct/ch03.html) of the [Immersive Linear Algebra online book](http://immersivemath.com/ila/index.html).\n",
    "\n",
    "<img src=\"images/projection_line.png\" alt=\"projection\" style=\"width: 70%\"/>\n",
    "(source: [Immersive Math](http://immersivemath.com/ila/ch03_dotproduct/ch03.html))\n",
    "\n",
    "And here is what it looks like to project a vector onto a plane:\n",
    "\n",
    "<img src=\"images/projection.png\" alt=\"projection\" style=\"width: 70%\"/>\n",
    "(source: [The Linear Algebra View of Least-Squares Regression](https://medium.com/@andrew.chamberlain/the-linear-algebra-view-of-least-squares-regression-f67044b7f39b))\n",
    "\n",
    "When vector $\\mathbf{b}$ is projected onto a line $\\mathbf{a}$, its projection $\\mathbf{p}$ is the part of $\\mathbf{b}$ along that line $\\mathbf{a}$.  So $\\mathbf{p}$ is some multiple of $\\mathbf{a}$. Let $\\mathbf{p} = \\hat{x}\\mathbf{a}$ where $\\hat{x}$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Orthogonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**The key to projection is orthogonality:** The line *from* $\\mathbf{b}$ to $\\mathbf{p}$ (which can be written $\\mathbf{b} - \\hat{x}\\mathbf{a}$) is perpendicular to $\\mathbf{a}$.\n",
    "\n",
    "This means that $$ \\mathbf{a} \\cdot (\\mathbf{b} -  \\hat{x}\\mathbf{a}) = 0 $$\n",
    "\n",
    "and so $$\\hat{x} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\mathbf{a} \\cdot \\mathbf{a}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Motivation**:\n",
    "\n",
    "We want orthonormal columns in $Q$ and a Hessenberg $H$ such that $A Q = Q H$.\n",
    "\n",
    "Thinking about it iteratively, $$ A Q_n = Q_{n+1} H_n $$ where $Q_{n+1}$ is $n\\times n+1$ and $H_n$ is $n+1 \\times n$.  This creates a solvable recurrence relation.\n",
    "\n",
    "<img src=\"images/arnoldi.jpg\" alt=\"arnoldi\" style=\"width: 95%\"/>\n",
    "(source: Trefethen, Lecture 33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Pseudo-code for Arnoldi Algorithm**\n",
    "\n",
    "    Start with an arbitrary vector (normalized to have norm 1) for first col of Q\n",
    "    for n=1,2,3...\n",
    "        v = A @ nth col of Q\n",
    "        for j=1,...n\n",
    "            project v onto q_j, and subtract the projection off of v\n",
    "            want to capture part of v that isn't already spanned by prev columns of Q\n",
    "            store coefficients in H\n",
    "        normalize v, and then make it the (n+1)th column of Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice that we are multiplying A by the previous vector in Q and removing the components that are not orthogonal to the existing columns of Q.\n",
    "\n",
    "**Question:** Repeated multiplications of A?  Does this remind you of anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Exercise Answer\n",
    "The *Power Method* involved iterative multiplications by A as well!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### About how the Arnoldi Iteration works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- With the Arnoldi Iteration, we are finding an orthonormal basis for the *Krylov subspace*. \n",
    "The Krylov matrix $$ K = \\left[b \\; Ab \\; A^2b \\; \\dots \\; A^{n-1}b \\right]$$\n",
    "has a QR factorization\n",
    "$$K = QR$$\n",
    "and that is the same $Q$ that is being found in the Arnoldi Iteration.  Note that the Arnoldi Iteration does not explicity calculate $K$ or $R$.\n",
    "\n",
    "- Intuition: K contains good information about the largest eigenvalues of A, and the QR factorization reveals this information by peeling off one approximate eigenvector at a time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Arnoldi Iteration is two things:\n",
    "1. the basis of many of the iterative algorithms of numerical linear algebra\n",
    "2. a technique for finding eigenvalues of nonhermitian matrices\n",
    "(Trefethen, page 257)\n",
    "\n",
    "**How Arnoldi Locates Eigenvalues**\n",
    "\n",
    "1. Carry out Arnoldi iteration\n",
    "2. Periodically calculate the eigenvalues (called *Arnoldi estimates* or *Ritz values*) of the Hessenberg H, using the QR algorithm\n",
    "3. Check at whether these values are converging.  If they are, they're probably eigenvalues of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Decompose square matrix A @ Q ~= Q @ H\n",
    "def arnoldi(A):\n",
    "    m, n = A.shape\n",
    "    assert(n <= m)\n",
    "    \n",
    "    # Hessenberg matrix\n",
    "    H = np.zeros([n+1,n]) #, dtype=np.float64)\n",
    "    # Orthonormal columns\n",
    "    Q = np.zeros([m,n+1]) #, dtype=np.float64)\n",
    "    # 1st col of Q is a random column with unit norm\n",
    "    b = np.random.rand(m)\n",
    "    Q[:,0] = b / np.linalg.norm(b)\n",
    "    for j in range(n):\n",
    "        v = A @ Q[:,j]\n",
    "        for i in range(j+1):\n",
    "            #This comes from the formula for projection of v onto q.\n",
    "            #Since columns q are orthonormal, q dot q = 1\n",
    "            H[i,j] = np.dot(Q[:,i], v)\n",
    "            v = v - (H[i,j] * Q[:,i])\n",
    "        H[j+1,j] = np.linalg.norm(v)\n",
    "        Q[:,j+1] = v / H[j+1,j]\n",
    "        \n",
    "        # printing this to see convergence, would be slow to use in practice\n",
    "        print(np.linalg.norm(A @ Q[:,:-1] - Q @ H))\n",
    "    return Q[:,:-1], H[:-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Q, H = arnoldi(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check that H is tri-diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write code to confirm that:\n",
    "1. AQ = QH\n",
    "2. Q is orthonormal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Exercise:\n",
    "np.allclose(A @ Q, Q @ H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Exercise:\n",
    "np.allclose(np.eye(len(Q)), Q.T @ Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### General Case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**General Matrix**: Now we can do this on our general matrix A (not symmetric).  In this case, we are getting a Hessenberg instead of a Tri-diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Q0, H0 = arnoldi(A0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Check that H is Hessenberg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.allclose(A0 @ Q0, Q0 @ H0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.allclose(np.eye(len(Q0)), Q0.T @ Q0), np.allclose(np.eye(len(Q0)), Q0 @ Q0.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def eigen(A, max_iter=20):\n",
    "    Q, H = arnoldi(A)\n",
    "    Ak, QQ = practical_qr(H, max_iter)\n",
    "    U = Q @ QQ\n",
    "    D = np.diag(Ak)\n",
    "    return U, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "A0 = np.random.rand(n,n)\n",
    "A = A0 @ A0.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "U, D = eigen(A, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(U @ np.diag(D) @ U.T - A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.allclose(U @ np.diag(D) @ U.T, A, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's find some eigenvalues!\n",
    "\n",
    "\n",
    "from [Nonsymmetric Eigenvalue Problems](https://sites.math.washington.edu/~morrow/498_13/eigenvalues.pdf) chapter:\n",
    "\n",
    "Note that \"direct\" methods must still iterate, since finding eigenvalues is mathematically equivalent to finding zeros of polynomials, for which no noniterative methods can exist. We call a method direct if experience shows that it (nearly) never fails to converge in a\n",
    "fixed number of iterations.\n",
    "\n",
    "Iterative methods typically provide approximations only to a subset of the eigenvalues and eigenvectors and are usually run only long enough to get a few adequately accurate eigenvalues rather than a large number\n",
    "\n",
    "our ultimate algorithm: the shifted Hessenberg QR algorithm\n",
    "\n",
    "More reading:\n",
    "- [The Symmetric Eigenproblem and SVD](https://sites.math.washington.edu/~morrow/498_13/eigenvalues2.pdf)\n",
    "- [Iterative Methods for Eigenvalue Problems](https://sites.math.washington.edu/~morrow/498_13/eigenvalues3.pdf) Rayleigh-Ritz Method, Lanczos algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Coming Up\n",
    "\n",
    "We will be coding our own QR decomposition (two different ways!) in the future, but first we are going to see another way that the QR decomposition can be used: to calculate linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symmetric matrices come up naturally:\n",
    "- distance matrices\n",
    "- relationship matrices (Facebook or LinkedIn)\n",
    "- ODEs\n",
    "\n",
    "We will look at positive definite matrices, since that guarantees that all the eigenvalues are real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in the confusing language of NLA, the QR algorithm is *direct*, because you are making progress on all columns at once.  In other math/CS language, the QR algorithm is *iterative*, because it iteratively converges and never reaches an exact solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structured orthogonalization.  In the language of NLA, Arnoldi iteration is considered an *iterative* algorithm, because you could stop part way and have a few columns completed.\n",
    "\n",
    "a Gram-Schmidt style iteration for transforming a matrix to Hessenberg form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {
    "2987aec2ec494d2b9b31bdf93ba87cb6": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "84bffde89e894158860b175e986f4d61": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}